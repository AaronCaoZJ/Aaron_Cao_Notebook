# Pytorch 神经网络学习笔记

## 范数

范数听起来很像距离的度量。欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些 启发。事实上，欧几里得距离是一个 $L_2$​ 范数：
$$
\vert\vert\mathbf{x}\vert\vert_2=\sqrt{\sum_{i=1}^{n}{x
_i^2}}
$$

```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```

$L_1$ 范数，它表示为向量元素的绝对值之和：
$$
\vert\vert\mathbf{x}\vert\vert_1=\sum_{i=1}^{n}{\vert x
_i\vert}
$$
以上都是更一般的 $L_p$ 范数的特例：
$$
\vert\vert\mathbf{x}\vert\vert_p=(\sum_{i=1}^{n}{{\vert x
_i\vert}^p})^{1/p}
$$
类似的，矩阵 $\mathbf{X}\in\mathbb{R}^{n \times d}$ 的 Frobenius 范数是矩阵元素的平方和的平方根：
$$
\vert\vert\mathbf{X}\vert\vert_F=\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}x_{ij}^2}
$$

## 线性神经网络

### 线性回归

所有特征为向量 $\mathbf{x}\in\mathbb{R}^d$，所有权重为向量 $\mathbf{w}\in\mathbb{R}^d$，用点积形式来简洁地表达模型：
$$
\hat{y}=\mathbf{w}^T\mathbf{x}+b
$$
对于整个数据集的 n 个样本，有 $\mathbf{X}\in\mathbb{R}^{n \times d}$，每一行是一个样本，每一列是一种特征。$\hat{y}\in\mathbb{R}^n$

通过矩阵向量乘积表示：
$$
\hat{y}=\mathbf{X}\mathbf{w}+b
$$

#### 损失函数

希望寻找一组参数 $(\mathbf{w}^*,b^*)$ 最小化所有训练样本的总损失：
$$
\mathbf{w}^*,b^*={\underset{\mathbf{w},b}{\arg\min}\ L(\mathbf{w},b)}
$$

#### 正态分布和平方损失

随机变量 $x$ 具有均值 $\mu$ 和方差 $\sigma^2$，正态分布表示为：
$$
p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp{(-\frac{1}{2\sigma^2}(x-\mu)^2)}
$$

```python
def normal(x, mu, sigma):
p = 1 / math.sqrt(2 * math.pi * sigma**2)
return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)
```

均方误差损失函数可以用于线性回归的一个原因是：假设了观测中包含噪声，其中噪声服从正态分布 $\epsilon\sim\mathcal{N}(0,\sigma^2)$。
$$
y=\mathbf{w}^T\mathbf{x}+b+\epsilon
$$
可以写出通过给定的 $\mathbf{x}$ 观测到特定 $y$ 的似然：
$$
p(y\vert x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp{(-\frac{1}{2\sigma^2}(y-\mathbf{w}^T\mathbf{x}-b)^2)}
$$
根据极大似然估计法，参数 $\mathbf{w},b$ 的最优值是使整个数据集的似然最大的值：
$$
P(y\vert x)=\prod_{i=1}^{n}p(y^{(i)}\vert x^{(i)})
$$

$$
-\log{P(y\vert x)}=\sum_{i=1}^{n}\frac{1}{2}\log{2\pi\sigma^2}+\frac{1}{2\sigma^2}(y-\mathbf{w}^T\mathbf{x}-b)^2
$$

```python
def squared_loss(y_hat, y):
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

常数 $\frac{1}{2}$ 不会带来本质的差别，但是有利于计算导数后系数为 1。

#### 随机梯度下降

通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降，随机抽样一个小批量 $\mathcal{B}$，计算小批 量的平均损失关于模型参数的导数，将梯度乘以一个预先确定的正数η，并从 当前参数的值中减掉：
$$
(\mathbf{w},b)\leftarrow(\mathbf{w},b)-\frac{\eta}{\vert\mathcal{B}\vert}{\sum\limits_{i\in\mathcal{B}}}\partial_{(\mathbf{w},b)}l^{(i)}(\mathbf{w},b)
$$

```python
def sgd(params, lr, batch_size):
"""小批量随机梯度下降"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

### softmax 回归

![image-20240319150205321](assets/image-20240319150205321-1710831728152-4.png)

***softmax 函数能够将未规范化的预测变换为非负数并且总和为 1，同时让模型保持可导。***
$$
\hat{y}=softmax(\mathbf{o}),\ \hat{y_j}=\frac{\exp{o_j}}{\sum_k{\exp{o_k}}}
$$
***对比hardmax，softmax不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。***

小批量样本特征 $\mathbf{X}\in\mathbb{R}^{n \times d}$ ，权重 $\mathbf{W}\in\mathbb{R}^{n \times d}$ ，偏执 $\mathbf{b}\in\mathbb{	R}^{1\times q}$softmax 回归的矢量计算表达式为：
$$
\mathbf{O}=\mathbf{X}\mathbf{W}+\mathbf{b}
$$

$$
\mathbf{\hat{Y}}=softmax(\mathbf{O})
$$

#### 交叉熵损失函数（一般与 softmax 一同使用）

$$
l(\mathbf{y},\mathbf{\hat{y}})=-\sum_{j=1}^{q}y_j\log{\hat{y_j}}\\
=\log{\sum_{k=1}^q\exp{o_k}}-\sum_{j=1}^qy_jo_j
$$

$$
\partial_{o_j}l(\mathbf{y},\mathbf{\hat{y}})=\frac{\exp{o_j}}{\sum_{k=1}^q{\exp{o_k}}}=softmax(o)_j-y_j
$$

信息论的核心思想是量化数据中的信息内容。在信息论中，该数值被称为分布 $P$ 的熵。可以通过 以下方程得到：
$$
H[P]=\sum_j{-P(j)\log{P(j)}}
$$
们可以把交叉熵想象为“主观概率为 $Q$ 的观察者在看到根据概率 $P$ 生成的数据时的预期惊异”。

#### softmin？

![image-20240319153642716](assets/image-20240319153642716-1710833807731-6.png)
